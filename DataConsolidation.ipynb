{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Model training\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file was originally for frequency distributions for Latin names!\n",
    "but we can also concat the other dfs / do model training after in the same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indo = pd.read_pickle('df_indo.pkl.gz', compression='gzip')\n",
    "df_malay = pd.read_pickle('df_malay.pkl.gz', compression='gzip')\n",
    "df_viet = pd.read_csv('viet_df.csv')\n",
    "df_cnrom = pd.read_csv('cnrom_df.csv')\n",
    "df_cnchar = pd.read_csv('cnchar_df.csv')\n",
    "df_turk = pd.read_pickle('turkish_df.pkl.gz', compression='gzip')\n",
    "df_korean = pd.read_pickle('korean_df.pkl.gz', compression='gzip') \n",
    "# also import other dfs\n",
    "\n",
    "all_dfs = [df_indo, df_malay, df_viet, df_cnrom, df_cnchar, df_turk, df_korean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesian : 11246\n",
      "Malay : 2908\n",
      "Vietnamese : 2290\n",
      "Chinese (Romanized) : 10478\n",
      "Chinese (Characters) : 11055\n",
      "Turkish : 18037\n",
      "Korean (Romanized & Characters) : 19219\n"
     ]
    }
   ],
   "source": [
    "# finding percentages\n",
    "df_names = ['Indonesian', 'Malay', 'Vietnamese', 'Chinese (Romanized)', 'Chinese (Characters)', 'Turkish', 'Korean (Romanized & Characters)']\n",
    "total_size = 0\n",
    "\n",
    "for i, df in enumerate(all_dfs):\n",
    "    total_size += df.shape[0]\n",
    "    print(df_names[i], ':', df.shape[0])\n",
    "    \n",
    "# for i, df in enumerate(all_dfs):\n",
    "#     print(df_names[i], ':', df.shape[0] / total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning up column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column names to KEEP: (10 so far)\n",
    "\n",
    "* name_length\n",
    "* avg_token_length\n",
    "* num_tokens\n",
    "* period_freq\n",
    "* dash_freq\n",
    "* apostrophe_freq\n",
    "* space_freq\n",
    "* unigrams_cosine_sim\n",
    "* bigrams_cosine_sim\n",
    "* language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # finding what columns r in viet that aren't in indo\n",
    "# print(df_indo.columns)\n",
    "# print(df_viet.columns)\n",
    "# [col for col in df_viet.columns if col not in df_indo.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk if i can generalize this lol\n",
    "# def rename_columns(df_ref, df_to_change):\n",
    "#     diff_cols = [col for col in df_to_change.columns if col not in df_ref.columns]\n",
    "#     new_names = \n",
    "#     df_to_change.rename(columns = {oldName : newName})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>name_length</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>char_ngrams</th>\n",
       "      <th>period_freq</th>\n",
       "      <th>dash_freq</th>\n",
       "      <th>space_freq</th>\n",
       "      <th>apostrophe_freq</th>\n",
       "      <th>transliteration</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>indiv_unigrams_fdist</th>\n",
       "      <th>indiv_bigrams_fdist</th>\n",
       "      <th>unigrams_cosine_sim</th>\n",
       "      <th>bigrams_cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>từ hoàng thông</td>\n",
       "      <td>['LATIN', 'LATIN', 'SPACE', 'LATIN', 'LATIN', ...</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>[('T',), ('ừ',), (' ',), ('H',), ('o',), ('à',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>tu hoang thong</td>\n",
       "      <td>['t', 'ừ', ' ', 'h', 'o', 'à', 'n', 'g', ' ', ...</td>\n",
       "      <td>[('t', 'ừ'), ('ừ', ' '), (' ', 'h'), ('h', 'o'...</td>\n",
       "      <td>[('t', 'ừ', ' '), ('ừ', ' ', 'h'), (' ', 'h', ...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>[[0.14285714 0.         0.         0.         ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.805625</td>\n",
       "      <td>0.508198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nguyễn thị phương thảo</td>\n",
       "      <td>['LATIN', 'LATIN', 'LATIN', 'LATIN', 'LATIN', ...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>[('N',), ('g',), ('u',), ('y',), ('ễ',), ('n',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>nguyen thi phuong thao</td>\n",
       "      <td>['n', 'g', 'u', 'y', 'ễ', 'n', ' ', 't', 'h', ...</td>\n",
       "      <td>[('n', 'g'), ('g', 'u'), ('u', 'y'), ('y', 'ễ'...</td>\n",
       "      <td>[('n', 'g', 'u'), ('g', 'u', 'y'), ('u', 'y', ...</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>[[0.13636364 0.         0.         0.         ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.884792</td>\n",
       "      <td>0.667716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nick út</td>\n",
       "      <td>['LATIN', 'LATIN', 'LATIN', 'LATIN', 'SPACE', ...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>[('N',), ('i',), ('c',), ('k',), (' ',), ('Ú',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>nick ut</td>\n",
       "      <td>['n', 'i', 'c', 'k', ' ', 'ú', 't']</td>\n",
       "      <td>[('n', 'i'), ('i', 'c'), ('c', 'k'), ('k', ' '...</td>\n",
       "      <td>[('n', 'i', 'c'), ('i', 'c', 'k'), ('c', 'k', ...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>[[0.14285714 0.         0.         0.14285714 ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.592690</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cao văn lầu</td>\n",
       "      <td>['LATIN', 'LATIN', 'LATIN', 'SPACE', 'LATIN', ...</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>[('C',), ('a',), ('o',), (' ',), ('V',), ('ă',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>cao van lau</td>\n",
       "      <td>['c', 'a', 'o', ' ', 'v', 'ă', 'n', ' ', 'l', ...</td>\n",
       "      <td>[('c', 'a'), ('a', 'o'), ('o', ' '), (' ', 'v'...</td>\n",
       "      <td>[('c', 'a', 'o'), ('a', 'o', ' '), ('o', ' ', ...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>[[0.18181818 0.09090909 0.         0.09090909 ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.665965</td>\n",
       "      <td>0.243176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tạ thu thâu</td>\n",
       "      <td>['LATIN', 'LATIN', 'SPACE', 'LATIN', 'LATIN', ...</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>[('T',), ('ạ',), (' ',), ('T',), ('h',), ('u',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ta thu thau</td>\n",
       "      <td>['t', 'ạ', ' ', 't', 'h', 'u', ' ', 't', 'h', ...</td>\n",
       "      <td>[('t', 'ạ'), ('ạ', ' '), (' ', 't'), ('t', 'h'...</td>\n",
       "      <td>[('t', 'ạ', ' '), ('ạ', ' ', 't'), (' ', 't', ...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>[[0.18181818 0.         0.         0.         ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.596114</td>\n",
       "      <td>0.288942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>nguyen duc kien</td>\n",
       "      <td>['LATIN', 'LATIN', 'LATIN', 'LATIN', 'LATIN', ...</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>[('N',), ('g',), ('u',), ('y',), ('e',), ('n',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>nguyen duc kien</td>\n",
       "      <td>['n', 'g', 'u', 'y', 'e', 'n', ' ', 'd', 'u', ...</td>\n",
       "      <td>[('n', 'g'), ('g', 'u'), ('u', 'y'), ('y', 'e'...</td>\n",
       "      <td>[('n', 'g', 'u'), ('g', 'u', 'y'), ('u', 'y', ...</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>[[0.13333333 0.         0.         0.06666667 ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.765422</td>\n",
       "      <td>0.440203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>nguyen phuc thai</td>\n",
       "      <td>['LATIN', 'LATIN', 'LATIN', 'LATIN', 'LATIN', ...</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[('N',), ('g',), ('u',), ('y',), ('e',), ('n',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>nguyen phuc thai</td>\n",
       "      <td>['n', 'g', 'u', 'y', 'e', 'n', ' ', 'p', 'h', ...</td>\n",
       "      <td>[('n', 'g'), ('g', 'u'), ('u', 'y'), ('y', 'e'...</td>\n",
       "      <td>[('n', 'g', 'u'), ('g', 'u', 'y'), ('u', 'y', ...</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>[[0.125  0.0625 0.     0.0625 0.     0.0625 0....</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.901822</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>lê phổ</td>\n",
       "      <td>['LATIN', 'LATIN', 'SPACE', 'LATIN', 'LATIN', ...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[('L',), ('ê',), (' ',), ('P',), ('h',), ('ổ',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>le pho</td>\n",
       "      <td>['l', 'ê', ' ', 'p', 'h', 'ổ']</td>\n",
       "      <td>[('l', 'ê'), ('ê', ' '), (' ', 'p'), ('p', 'h'...</td>\n",
       "      <td>[('l', 'ê', ' '), ('ê', ' ', 'p'), (' ', 'p', ...</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>[[0.16666667 0.         0.         0.         ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.112297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>vu ngoc nha</td>\n",
       "      <td>['LATIN', 'LATIN', 'SPACE', 'LATIN', 'LATIN', ...</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>[('V',), ('u',), (' ',), ('N',), ('g',), ('o',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>vu ngoc nha</td>\n",
       "      <td>['v', 'u', ' ', 'n', 'g', 'o', 'c', ' ', 'n', ...</td>\n",
       "      <td>[('v', 'u'), ('u', ' '), (' ', 'n'), ('n', 'g'...</td>\n",
       "      <td>[('v', 'u', ' '), ('u', ' ', 'n'), (' ', 'n', ...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>[[0.18181818 0.09090909 0.         0.09090909 ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.890835</td>\n",
       "      <td>0.339892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>hoang ke viem</td>\n",
       "      <td>['LATIN', 'LATIN', 'LATIN', 'LATIN', 'LATIN', ...</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>[('H',), ('o',), ('a',), ('n',), ('g',), (' ',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>hoang ke viem</td>\n",
       "      <td>['h', 'o', 'a', 'n', 'g', ' ', 'k', 'e', ' ', ...</td>\n",
       "      <td>[('h', 'o'), ('o', 'a'), ('a', 'n'), ('n', 'g'...</td>\n",
       "      <td>[('h', 'o', 'a'), ('o', 'a', 'n'), ('a', 'n', ...</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>[[0.15384615 0.07692308 0.         0.         ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.710261</td>\n",
       "      <td>0.370277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2290 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    fullname  \\\n",
       "0             từ hoàng thông   \n",
       "1     nguyễn thị phương thảo   \n",
       "2                    nick út   \n",
       "3                cao văn lầu   \n",
       "4                tạ thu thâu   \n",
       "...                      ...   \n",
       "2285         nguyen duc kien   \n",
       "2286        nguyen phuc thai   \n",
       "2287                  lê phổ   \n",
       "2288             vu ngoc nha   \n",
       "2289           hoang ke viem   \n",
       "\n",
       "                                               alphabet  name_length  \\\n",
       "0     ['LATIN', 'LATIN', 'SPACE', 'LATIN', 'LATIN', ...           14   \n",
       "1     ['LATIN', 'LATIN', 'LATIN', 'LATIN', 'LATIN', ...           22   \n",
       "2     ['LATIN', 'LATIN', 'LATIN', 'LATIN', 'SPACE', ...            7   \n",
       "3     ['LATIN', 'LATIN', 'LATIN', 'SPACE', 'LATIN', ...           11   \n",
       "4     ['LATIN', 'LATIN', 'SPACE', 'LATIN', 'LATIN', ...           11   \n",
       "...                                                 ...          ...   \n",
       "2285  ['LATIN', 'LATIN', 'LATIN', 'LATIN', 'LATIN', ...           15   \n",
       "2286  ['LATIN', 'LATIN', 'LATIN', 'LATIN', 'LATIN', ...           16   \n",
       "2287  ['LATIN', 'LATIN', 'SPACE', 'LATIN', 'LATIN', ...            6   \n",
       "2288  ['LATIN', 'LATIN', 'SPACE', 'LATIN', 'LATIN', ...           11   \n",
       "2289  ['LATIN', 'LATIN', 'LATIN', 'LATIN', 'LATIN', ...           13   \n",
       "\n",
       "      num_tokens                                        char_ngrams  \\\n",
       "0              3  [('T',), ('ừ',), (' ',), ('H',), ('o',), ('à',...   \n",
       "1              4  [('N',), ('g',), ('u',), ('y',), ('ễ',), ('n',...   \n",
       "2              2  [('N',), ('i',), ('c',), ('k',), (' ',), ('Ú',...   \n",
       "3              3  [('C',), ('a',), ('o',), (' ',), ('V',), ('ă',...   \n",
       "4              3  [('T',), ('ạ',), (' ',), ('T',), ('h',), ('u',...   \n",
       "...          ...                                                ...   \n",
       "2285           3  [('N',), ('g',), ('u',), ('y',), ('e',), ('n',...   \n",
       "2286           3  [('N',), ('g',), ('u',), ('y',), ('e',), ('n',...   \n",
       "2287           2  [('L',), ('ê',), (' ',), ('P',), ('h',), ('ổ',...   \n",
       "2288           3  [('V',), ('u',), (' ',), ('N',), ('g',), ('o',...   \n",
       "2289           3  [('H',), ('o',), ('a',), ('n',), ('g',), (' ',...   \n",
       "\n",
       "      period_freq  dash_freq  space_freq  apostrophe_freq  \\\n",
       "0               0          0           2                0   \n",
       "1               0          0           3                0   \n",
       "2               0          0           1                0   \n",
       "3               0          0           2                0   \n",
       "4               0          0           2                0   \n",
       "...           ...        ...         ...              ...   \n",
       "2285            0          0           2                0   \n",
       "2286            0          0           2                0   \n",
       "2287            0          0           1                0   \n",
       "2288            0          0           2                0   \n",
       "2289            0          0           2                0   \n",
       "\n",
       "             transliteration  \\\n",
       "0             tu hoang thong   \n",
       "1     nguyen thi phuong thao   \n",
       "2                    nick ut   \n",
       "3                cao van lau   \n",
       "4                ta thu thau   \n",
       "...                      ...   \n",
       "2285         nguyen duc kien   \n",
       "2286        nguyen phuc thai   \n",
       "2287                  le pho   \n",
       "2288             vu ngoc nha   \n",
       "2289           hoang ke viem   \n",
       "\n",
       "                                               unigrams  \\\n",
       "0     ['t', 'ừ', ' ', 'h', 'o', 'à', 'n', 'g', ' ', ...   \n",
       "1     ['n', 'g', 'u', 'y', 'ễ', 'n', ' ', 't', 'h', ...   \n",
       "2                   ['n', 'i', 'c', 'k', ' ', 'ú', 't']   \n",
       "3     ['c', 'a', 'o', ' ', 'v', 'ă', 'n', ' ', 'l', ...   \n",
       "4     ['t', 'ạ', ' ', 't', 'h', 'u', ' ', 't', 'h', ...   \n",
       "...                                                 ...   \n",
       "2285  ['n', 'g', 'u', 'y', 'e', 'n', ' ', 'd', 'u', ...   \n",
       "2286  ['n', 'g', 'u', 'y', 'e', 'n', ' ', 'p', 'h', ...   \n",
       "2287                     ['l', 'ê', ' ', 'p', 'h', 'ổ']   \n",
       "2288  ['v', 'u', ' ', 'n', 'g', 'o', 'c', ' ', 'n', ...   \n",
       "2289  ['h', 'o', 'a', 'n', 'g', ' ', 'k', 'e', ' ', ...   \n",
       "\n",
       "                                                bigrams  \\\n",
       "0     [('t', 'ừ'), ('ừ', ' '), (' ', 'h'), ('h', 'o'...   \n",
       "1     [('n', 'g'), ('g', 'u'), ('u', 'y'), ('y', 'ễ'...   \n",
       "2     [('n', 'i'), ('i', 'c'), ('c', 'k'), ('k', ' '...   \n",
       "3     [('c', 'a'), ('a', 'o'), ('o', ' '), (' ', 'v'...   \n",
       "4     [('t', 'ạ'), ('ạ', ' '), (' ', 't'), ('t', 'h'...   \n",
       "...                                                 ...   \n",
       "2285  [('n', 'g'), ('g', 'u'), ('u', 'y'), ('y', 'e'...   \n",
       "2286  [('n', 'g'), ('g', 'u'), ('u', 'y'), ('y', 'e'...   \n",
       "2287  [('l', 'ê'), ('ê', ' '), (' ', 'p'), ('p', 'h'...   \n",
       "2288  [('v', 'u'), ('u', ' '), (' ', 'n'), ('n', 'g'...   \n",
       "2289  [('h', 'o'), ('o', 'a'), ('a', 'n'), ('n', 'g'...   \n",
       "\n",
       "                                               trigrams  avg_token_length  \\\n",
       "0     [('t', 'ừ', ' '), ('ừ', ' ', 'h'), (' ', 'h', ...          4.000000   \n",
       "1     [('n', 'g', 'u'), ('g', 'u', 'y'), ('u', 'y', ...          4.750000   \n",
       "2     [('n', 'i', 'c'), ('i', 'c', 'k'), ('c', 'k', ...          3.000000   \n",
       "3     [('c', 'a', 'o'), ('a', 'o', ' '), ('o', ' ', ...          3.000000   \n",
       "4     [('t', 'ạ', ' '), ('ạ', ' ', 't'), (' ', 't', ...          3.000000   \n",
       "...                                                 ...               ...   \n",
       "2285  [('n', 'g', 'u'), ('g', 'u', 'y'), ('u', 'y', ...          4.333333   \n",
       "2286  [('n', 'g', 'u'), ('g', 'u', 'y'), ('u', 'y', ...          4.666667   \n",
       "2287  [('l', 'ê', ' '), ('ê', ' ', 'p'), (' ', 'p', ...          2.500000   \n",
       "2288  [('v', 'u', ' '), ('u', ' ', 'n'), (' ', 'n', ...          3.000000   \n",
       "2289  [('h', 'o', 'a'), ('o', 'a', 'n'), ('a', 'n', ...          3.666667   \n",
       "\n",
       "                                   indiv_unigrams_fdist  \\\n",
       "0     [[0.14285714 0.         0.         0.         ...   \n",
       "1     [[0.13636364 0.         0.         0.         ...   \n",
       "2     [[0.14285714 0.         0.         0.14285714 ...   \n",
       "3     [[0.18181818 0.09090909 0.         0.09090909 ...   \n",
       "4     [[0.18181818 0.         0.         0.         ...   \n",
       "...                                                 ...   \n",
       "2285  [[0.13333333 0.         0.         0.06666667 ...   \n",
       "2286  [[0.125  0.0625 0.     0.0625 0.     0.0625 0....   \n",
       "2287  [[0.16666667 0.         0.         0.         ...   \n",
       "2288  [[0.18181818 0.09090909 0.         0.09090909 ...   \n",
       "2289  [[0.15384615 0.07692308 0.         0.         ...   \n",
       "\n",
       "            indiv_bigrams_fdist  unigrams_cosine_sim  bigrams_cosine_sim  \n",
       "0     [[0. 0. 0. ... 0. 0. 0.]]             0.805625            0.508198  \n",
       "1     [[0. 0. 0. ... 0. 0. 0.]]             0.884792            0.667716  \n",
       "2     [[0. 0. 0. ... 0. 0. 0.]]             0.592690            0.005600  \n",
       "3     [[0. 0. 0. ... 0. 0. 0.]]             0.665965            0.243176  \n",
       "4     [[0. 0. 0. ... 0. 0. 0.]]             0.596114            0.288942  \n",
       "...                         ...                  ...                 ...  \n",
       "2285  [[0. 0. 0. ... 0. 0. 0.]]             0.765422            0.440203  \n",
       "2286  [[0. 0. 0. ... 0. 0. 0.]]             0.901822            0.607800  \n",
       "2287  [[0. 0. 0. ... 0. 0. 0.]]             0.428565            0.112297  \n",
       "2288  [[0. 0. 0. ... 0. 0. 0.]]             0.890835            0.339892  \n",
       "2289  [[0. 0. 0. ... 0. 0. 0.]]             0.710261            0.370277  \n",
       "\n",
       "[2290 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is where you rename columns to all match\n",
    "df_viet.rename(columns = {'word_length': 'name_length'}, inplace = True)\n",
    "df_cnrom.rename(columns = {'word_length': 'name_length'}, inplace = True)\n",
    "df_cnchar.rename(columns = {'word_length': 'name_length'}, inplace = True)\n",
    "df_viet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Redoing frequency distributions across all Latin names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Frequency Distribution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from IndoMalay.ipynb\n",
    "\n",
    "def create_lang_char_distribution(df, col_name):\n",
    "    char_freqs = {}\n",
    "    total_num_chars = 0  # across the entire language/dataset\n",
    "\n",
    "    for name in df[col_name]:\n",
    "        for char in name:\n",
    "            if char not in char_freqs.keys():\n",
    "                char_freqs[char] = 1\n",
    "            else:\n",
    "                char_freqs[char] += 1\n",
    "            total_num_chars += 1\n",
    "\n",
    "    char_freqs_relative = dict(sorted({char: count / total_num_chars for char, count in char_freqs.items()}.items()))\n",
    "    return char_freqs_relative\n",
    "\n",
    "def initialize_all_possible_bigrams(all_possible_chars):\n",
    "    all_possible_bigrams = {}\n",
    "    for first_char in all_possible_chars:  # first character of the current bigram\n",
    "        for second_char in all_possible_chars:  # second character of the current bigram\n",
    "            all_possible_bigrams[(first_char, second_char)] = 0\n",
    "    return all_possible_bigrams\n",
    "\n",
    "def create_lang_gram_distribution(initialized_grams, df, col_name):\n",
    "    gram_freqs = initialized_grams.copy()  # need a copy otherwise initiailized_grams is changed\n",
    "    total_num_grams = 0  # across the entire language/dataset\n",
    "    \n",
    "    for grams_list in df[col_name]:\n",
    "        for gram in grams_list:\n",
    "            gram_freqs[gram] += 1\n",
    "            total_num_grams += 1\n",
    "    \n",
    "    gram_freqs_relative = {gram: count / total_num_grams for gram, count in gram_freqs.items()}\n",
    "    return gram_freqs_relative\n",
    "\n",
    "def initialize_all_possible_trigrams(all_possible_chars):\n",
    "    all_possible_trigrams = {}\n",
    "    for first_char in all_possible_chars:  # first character of the current trigram\n",
    "        for second_char in all_possible_chars:  # second character of the current trigram\n",
    "            for third_char in all_possible_chars:  # third character of the current trigram\n",
    "                all_possible_trigrams[(first_char, second_char, third_char)] = 0\n",
    "    return all_possible_trigrams\n",
    "\n",
    "def create_indiv_gram_distribution(grams_list, initialized_grams):\n",
    "    gram_freqs_relative = initialized_grams.copy()  \n",
    "    num_grams = len(grams_list)  # for this current example\n",
    "    \n",
    "    for gram in grams_list:\n",
    "        gram_freqs_relative[gram] += 1 / num_grams\n",
    "\n",
    "    return gram_freqs_relative\n",
    "\n",
    "def set_indiv_trigram_dist(trigrams_list, init_trigrams):\n",
    "    trigrams_fdist_relative = init_trigrams\n",
    "    num_grams = len(trigrams_list)\n",
    "\n",
    "    for gram in trigrams_list:\n",
    "        trigrams_fdist_relative[gram] += 1 / num_grams\n",
    "\n",
    "    return trigrams_fdist_relative\n",
    "\n",
    "# TRIGRAMS individual frequency distributions\n",
    "#df_indo['indiv_trigrams_fdist'] = df_indo.apply(lambda row: set_indiv_trigram_dist(row['trigrams'], row['indiv_trigrams_fdist']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Determining which languages use Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these lines of code to work, the datasets must have been pickled to preserve data types! `pd.csv` turns everything into strings; for example, a list of `[LATIN, LATIN, LATIN, ...]` becomes `'[LATIN, LATIN, LATIN, ...]'` (i.e., `'['` becomes a character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indo_latin_percent = create_lang_char_distribution(df_indo, 'alphabet')['LATIN']\n",
    "malay_latin_percent = create_lang_char_distribution(df_malay, 'alphabet')['LATIN']\n",
    "#malay_latin_percent\n",
    "# viet_latin_percent = create_lang_char_distribution(df_viet, 'alphabet')['LATIN']\n",
    "# cnrom_latin_percent = create_lang_char_distribution(df_cnrom, 'alphabet')['LATIN']\n",
    "# cnchar_latin_percent = create_lang_char_distribution(df_cnchar, 'alphabet')['LATIN']\n",
    "turk_latin_percent = create_lang_char_distribution(df_turk, 'alphabet')['LATIN']\n",
    "korean_latin_percent = create_lang_char_distribution(df_korean, 'alphabet')['LATIN']\n",
    "\n",
    "#print(turk_latin_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. Remaking Frequency Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add a_hat_freq and turn categorical columns into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD after Maker Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see 10/16 anna meeting notes for ideas on more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cleaning up other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fullname', 'original_fullname', 'alphabet', 'transliteration',\n",
       "       'unigrams', 'bigrams', 'trigrams', 'char_ngrams', 'num_tokens',\n",
       "       'period_freq', 'dash_freq', 'space_freq', 'name_length',\n",
       "       'avg_token_length', 'indiv_unigrams_fdist', 'indiv_bigrams_fdist',\n",
       "       'unigrams_cosine_sim', 'bigrams_cosine_sim'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_korean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'label_tr', 'original_fullname', 'fullname', 'alphabet',\n",
       "       'unigrams', 'bigrams', 'trigrams', 'char_ngrams', 'name_length',\n",
       "       'num_tokens', 'avg_token_length', 'period_freq', 'dash_freq',\n",
       "       'space_freq', 'transliteration', 'indiv_unigrams_fdist',\n",
       "       'indiv_bigrams_fdist', 'indiv_trigrams_fdist', 'unigrams_cosine_sim',\n",
       "       'bigrams_cosine_sim', 'trigrams_cosine_sim'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_turk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turk['apostrophe_freq'] = df_turk['fullname'].apply(lambda name: name.count('\\''))\n",
    "df_korean['apostrophe_freq'] = df_korean['fullname'].apply(lambda name: name.count('\\''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Adding the language (label) to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indo['language'] = 'Indonesian'\n",
    "df_malay['language'] = 'Malay'\n",
    "df_viet['language'] = 'Vietnamese'\n",
    "df_cnrom['language'] = 'Chinese (Romanized)'\n",
    "df_cnchar['language'] = 'Chinese (Characters)'\n",
    "df_turk['language'] = 'Turkish'\n",
    "df_korean['language'] = 'Korean' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Combining all names to make one big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>original_fullname</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>char_ngrams</th>\n",
       "      <th>word_ngrams</th>\n",
       "      <th>name_length</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>space_freq</th>\n",
       "      <th>indiv_unigrams_fdist</th>\n",
       "      <th>indiv_bigrams_fdist</th>\n",
       "      <th>indiv_trigrams_fdist</th>\n",
       "      <th>unigrams_cosine_sim</th>\n",
       "      <th>bigrams_cosine_sim</th>\n",
       "      <th>trigrams_cosine_sim</th>\n",
       "      <th>language</th>\n",
       "      <th>id</th>\n",
       "      <th>label_tr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supriyadi</td>\n",
       "      <td>Supriyadi</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[s, u, p, r, i, y, a, d, i]</td>\n",
       "      <td>[(s, u), (u, p), (p, r), (r, i), (i, y), (y, a...</td>\n",
       "      <td>[(s, u, p), (u, p, r), (p, r, i), (r, i, y), (...</td>\n",
       "      <td>[s, u, p, r, i, y, a, d, i, (s, u), (u, p), (p...</td>\n",
       "      <td>[supriyadi]</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.664809</td>\n",
       "      <td>0.250640</td>\n",
       "      <td>0.085949</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>triyaningsih</td>\n",
       "      <td>Triyaningsih</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[t, r, i, y, a, n, i, n, g, s, i, h]</td>\n",
       "      <td>[(t, r), (r, i), (i, y), (y, a), (a, n), (n, i...</td>\n",
       "      <td>[(t, r, i), (r, i, y), (i, y, a), (y, a, n), (...</td>\n",
       "      <td>[t, r, i, y, a, n, i, n, g, s, i, h, (t, r), (...</td>\n",
       "      <td>[triyaningsih]</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.686625</td>\n",
       "      <td>0.353292</td>\n",
       "      <td>0.117226</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soerjadi</td>\n",
       "      <td>Soerjadi</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[s, o, e, r, j, a, d, i]</td>\n",
       "      <td>[(s, o), (o, e), (e, r), (r, j), (j, a), (a, d...</td>\n",
       "      <td>[(s, o, e), (o, e, r), (e, r, j), (r, j, a), (...</td>\n",
       "      <td>[s, o, e, r, j, a, d, i, (s, o), (o, e), (e, r...</td>\n",
       "      <td>[soerjadi]</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0....</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.197139</td>\n",
       "      <td>0.090295</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>undunsyah</td>\n",
       "      <td>Undunsyah</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[u, n, d, u, n, s, y, a, h]</td>\n",
       "      <td>[(u, n), (n, d), (d, u), (u, n), (n, s), (s, y...</td>\n",
       "      <td>[(u, n, d), (n, d, u), (d, u, n), (u, n, s), (...</td>\n",
       "      <td>[u, n, d, u, n, s, y, a, h, (u, n), (n, d), (d...</td>\n",
       "      <td>[undunsyah]</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.581396</td>\n",
       "      <td>0.155386</td>\n",
       "      <td>0.060083</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soeripto</td>\n",
       "      <td>Soeripto</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[s, o, e, r, i, p, t, o]</td>\n",
       "      <td>[(s, o), (o, e), (e, r), (r, i), (i, p), (p, t...</td>\n",
       "      <td>[(s, o, e), (o, e, r), (e, r, i), (r, i, p), (...</td>\n",
       "      <td>[s, o, e, r, i, p, t, o, (s, o), (o, e), (e, r...</td>\n",
       "      <td>[soeripto]</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.463215</td>\n",
       "      <td>0.176917</td>\n",
       "      <td>0.052811</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75228</th>\n",
       "      <td>lee han-wi</td>\n",
       "      <td>Lee Han-wi</td>\n",
       "      <td>[LATIN, LATIN, LATIN, SPACE, LATIN, LATIN, LAT...</td>\n",
       "      <td>[l, e, e,  , h, a, n, -, w, i]</td>\n",
       "      <td>[(l, e), (e, e), (e,  ), ( , h), (h, a), (a, n...</td>\n",
       "      <td>[(l, e, e), (e, e,  ), (e,  , h), ( , h, a), (...</td>\n",
       "      <td>[l, e, e,  , h, a, n, -, w, i, (l, e), (e, e),...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.675708</td>\n",
       "      <td>0.213562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75229</th>\n",
       "      <td>gil jung-woo</td>\n",
       "      <td>Gil Jung-woo</td>\n",
       "      <td>[LATIN, LATIN, LATIN, SPACE, LATIN, LATIN, LAT...</td>\n",
       "      <td>[g, i, l,  , j, u, n, g, -, w, o, o]</td>\n",
       "      <td>[(g, i), (i, l), (l,  ), ( , j), (j, u), (u, n...</td>\n",
       "      <td>[(g, i, l), (i, l,  ), (l,  , j), ( , j, u), (...</td>\n",
       "      <td>[g, i, l,  , j, u, n, g, -, w, o, o, (g, i), (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770059</td>\n",
       "      <td>0.395635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75230</th>\n",
       "      <td>이정희</td>\n",
       "      <td>이정희</td>\n",
       "      <td>[HANGUL, HANGUL, HANGUL]</td>\n",
       "      <td>[i, j, e, o, n, g, h, u, i]</td>\n",
       "      <td>[(i, j), (j, e), (e, o), (o, n), (n, g), (g, h...</td>\n",
       "      <td>[(i, j, e), (j, e, o), (e, o, n), (o, n, g), (...</td>\n",
       "      <td>[i, j, e, o, n, g, h, u, i, (i, j), (j, e), (e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.777063</td>\n",
       "      <td>0.507806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75231</th>\n",
       "      <td>금태섭</td>\n",
       "      <td>금태섭</td>\n",
       "      <td>[HANGUL, HANGUL, HANGUL]</td>\n",
       "      <td>[g, e, u, m, t, a, e, s, e, o, b]</td>\n",
       "      <td>[(g, e), (e, u), (u, m), (m, t), (t, a), (a, e...</td>\n",
       "      <td>[(g, e, u), (e, u, m), (u, m, t), (m, t, a), (...</td>\n",
       "      <td>[g, e, u, m, t, a, e, s, e, o, b, (g, e), (e, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.614455</td>\n",
       "      <td>0.228766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75232</th>\n",
       "      <td>oh ui-sik</td>\n",
       "      <td>Oh Ui-sik</td>\n",
       "      <td>[LATIN, LATIN, SPACE, LATIN, LATIN, HYPHEN-MIN...</td>\n",
       "      <td>[o, h,  , u, i, -, s, i, k]</td>\n",
       "      <td>[(o, h), (h,  ), ( , u), (u, i), (i, -), (-, s...</td>\n",
       "      <td>[(o, h,  ), (h,  , u), ( , u, i), (u, i, -), (...</td>\n",
       "      <td>[o, h,  , u, i, -, s, i, k, (o, h), (h,  ), ( ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.11...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.592440</td>\n",
       "      <td>0.067646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75233 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           fullname original_fullname  \\\n",
       "0         supriyadi         Supriyadi   \n",
       "1      triyaningsih      Triyaningsih   \n",
       "2          soerjadi          Soerjadi   \n",
       "3         undunsyah         Undunsyah   \n",
       "4          soeripto          Soeripto   \n",
       "...             ...               ...   \n",
       "75228    lee han-wi        Lee Han-wi   \n",
       "75229  gil jung-woo      Gil Jung-woo   \n",
       "75230           이정희               이정희   \n",
       "75231           금태섭               금태섭   \n",
       "75232     oh ui-sik         Oh Ui-sik   \n",
       "\n",
       "                                                alphabet  \\\n",
       "0      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "1      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "2      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "3      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "4      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "...                                                  ...   \n",
       "75228  [LATIN, LATIN, LATIN, SPACE, LATIN, LATIN, LAT...   \n",
       "75229  [LATIN, LATIN, LATIN, SPACE, LATIN, LATIN, LAT...   \n",
       "75230                           [HANGUL, HANGUL, HANGUL]   \n",
       "75231                           [HANGUL, HANGUL, HANGUL]   \n",
       "75232  [LATIN, LATIN, SPACE, LATIN, LATIN, HYPHEN-MIN...   \n",
       "\n",
       "                                   unigrams  \\\n",
       "0               [s, u, p, r, i, y, a, d, i]   \n",
       "1      [t, r, i, y, a, n, i, n, g, s, i, h]   \n",
       "2                  [s, o, e, r, j, a, d, i]   \n",
       "3               [u, n, d, u, n, s, y, a, h]   \n",
       "4                  [s, o, e, r, i, p, t, o]   \n",
       "...                                     ...   \n",
       "75228        [l, e, e,  , h, a, n, -, w, i]   \n",
       "75229  [g, i, l,  , j, u, n, g, -, w, o, o]   \n",
       "75230           [i, j, e, o, n, g, h, u, i]   \n",
       "75231     [g, e, u, m, t, a, e, s, e, o, b]   \n",
       "75232           [o, h,  , u, i, -, s, i, k]   \n",
       "\n",
       "                                                 bigrams  \\\n",
       "0      [(s, u), (u, p), (p, r), (r, i), (i, y), (y, a...   \n",
       "1      [(t, r), (r, i), (i, y), (y, a), (a, n), (n, i...   \n",
       "2      [(s, o), (o, e), (e, r), (r, j), (j, a), (a, d...   \n",
       "3      [(u, n), (n, d), (d, u), (u, n), (n, s), (s, y...   \n",
       "4      [(s, o), (o, e), (e, r), (r, i), (i, p), (p, t...   \n",
       "...                                                  ...   \n",
       "75228  [(l, e), (e, e), (e,  ), ( , h), (h, a), (a, n...   \n",
       "75229  [(g, i), (i, l), (l,  ), ( , j), (j, u), (u, n...   \n",
       "75230  [(i, j), (j, e), (e, o), (o, n), (n, g), (g, h...   \n",
       "75231  [(g, e), (e, u), (u, m), (m, t), (t, a), (a, e...   \n",
       "75232  [(o, h), (h,  ), ( , u), (u, i), (i, -), (-, s...   \n",
       "\n",
       "                                                trigrams  \\\n",
       "0      [(s, u, p), (u, p, r), (p, r, i), (r, i, y), (...   \n",
       "1      [(t, r, i), (r, i, y), (i, y, a), (y, a, n), (...   \n",
       "2      [(s, o, e), (o, e, r), (e, r, j), (r, j, a), (...   \n",
       "3      [(u, n, d), (n, d, u), (d, u, n), (u, n, s), (...   \n",
       "4      [(s, o, e), (o, e, r), (e, r, i), (r, i, p), (...   \n",
       "...                                                  ...   \n",
       "75228  [(l, e, e), (e, e,  ), (e,  , h), ( , h, a), (...   \n",
       "75229  [(g, i, l), (i, l,  ), (l,  , j), ( , j, u), (...   \n",
       "75230  [(i, j, e), (j, e, o), (e, o, n), (o, n, g), (...   \n",
       "75231  [(g, e, u), (e, u, m), (u, m, t), (m, t, a), (...   \n",
       "75232  [(o, h,  ), (h,  , u), ( , u, i), (u, i, -), (...   \n",
       "\n",
       "                                             char_ngrams     word_ngrams  \\\n",
       "0      [s, u, p, r, i, y, a, d, i, (s, u), (u, p), (p...     [supriyadi]   \n",
       "1      [t, r, i, y, a, n, i, n, g, s, i, h, (t, r), (...  [triyaningsih]   \n",
       "2      [s, o, e, r, j, a, d, i, (s, o), (o, e), (e, r...      [soerjadi]   \n",
       "3      [u, n, d, u, n, s, y, a, h, (u, n), (n, d), (d...     [undunsyah]   \n",
       "4      [s, o, e, r, i, p, t, o, (s, o), (o, e), (e, r...      [soeripto]   \n",
       "...                                                  ...             ...   \n",
       "75228  [l, e, e,  , h, a, n, -, w, i, (l, e), (e, e),...             NaN   \n",
       "75229  [g, i, l,  , j, u, n, g, -, w, o, o, (g, i), (...             NaN   \n",
       "75230  [i, j, e, o, n, g, h, u, i, (i, j), (j, e), (e...             NaN   \n",
       "75231  [g, e, u, m, t, a, e, s, e, o, b, (g, e), (e, ...             NaN   \n",
       "75232  [o, h,  , u, i, -, s, i, k, (o, h), (h,  ), ( ...             NaN   \n",
       "\n",
       "       name_length  avg_token_length  ...  space_freq  \\\n",
       "0                9               9.0  ...           0   \n",
       "1               12              12.0  ...           0   \n",
       "2                8               8.0  ...           0   \n",
       "3                9               9.0  ...           0   \n",
       "4                8               8.0  ...           0   \n",
       "...            ...               ...  ...         ...   \n",
       "75228           10               4.5  ...           1   \n",
       "75229           12               5.5  ...           1   \n",
       "75230            9               9.0  ...           0   \n",
       "75231           11              11.0  ...           0   \n",
       "75232            9               4.0  ...           1   \n",
       "\n",
       "                                    indiv_unigrams_fdist  \\\n",
       "0      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111,...   \n",
       "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333...   \n",
       "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0....   \n",
       "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111,...   \n",
       "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "...                                                  ...   \n",
       "75228  [[0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0,...   \n",
       "75229  [[0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "75230  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "75231  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "75232  [[0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.11...   \n",
       "\n",
       "                                     indiv_bigrams_fdist  \\\n",
       "0      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "...                                                  ...   \n",
       "75228  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "75229  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "75230  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "75231  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "75232  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                    indiv_trigrams_fdist  unigrams_cosine_sim  \\\n",
       "0      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.664809   \n",
       "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.686625   \n",
       "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.688312   \n",
       "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.581396   \n",
       "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.463215   \n",
       "...                                                  ...                  ...   \n",
       "75228                                                NaN             0.675708   \n",
       "75229                                                NaN             0.770059   \n",
       "75230                                                NaN             0.777063   \n",
       "75231                                                NaN             0.614455   \n",
       "75232                                                NaN             0.592440   \n",
       "\n",
       "       bigrams_cosine_sim trigrams_cosine_sim    language   id  label_tr  \n",
       "0                0.250640            0.085949  Indonesian  NaN       NaN  \n",
       "1                0.353292            0.117226  Indonesian  NaN       NaN  \n",
       "2                0.197139            0.090295  Indonesian  NaN       NaN  \n",
       "3                0.155386            0.060083  Indonesian  NaN       NaN  \n",
       "4                0.176917            0.052811  Indonesian  NaN       NaN  \n",
       "...                   ...                 ...         ...  ...       ...  \n",
       "75228            0.213562                 NaN      Korean  NaN       NaN  \n",
       "75229            0.395635                 NaN      Korean  NaN       NaN  \n",
       "75230            0.507806                 NaN      Korean  NaN       NaN  \n",
       "75231            0.228766                 NaN      Korean  NaN       NaN  \n",
       "75232            0.067646                 NaN      Korean  NaN       NaN  \n",
       "\n",
       "[75233 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see from output, we need the columns in the concatenated df (in this case, viet) to match\n",
    "# it's okay if some values are NaN bc we'll drop all non-numerical columns anyway\n",
    "merged_df = pd.concat(all_dfs, ignore_index = True, join = 'outer')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Keeping numerical columns only for each dataset (Same process as step 4, except we don't have to repeat lines of code...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_length</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>period_freq</th>\n",
       "      <th>dash_freq</th>\n",
       "      <th>apostrophe_freq</th>\n",
       "      <th>space_freq</th>\n",
       "      <th>unigrams_cosine_sim</th>\n",
       "      <th>bigrams_cosine_sim</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664809</td>\n",
       "      <td>0.250640</td>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.686625</td>\n",
       "      <td>0.353292</td>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.197139</td>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.581396</td>\n",
       "      <td>0.155386</td>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463215</td>\n",
       "      <td>0.176917</td>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75228</th>\n",
       "      <td>10</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.675708</td>\n",
       "      <td>0.213562</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75229</th>\n",
       "      <td>12</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.770059</td>\n",
       "      <td>0.395635</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75230</th>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777063</td>\n",
       "      <td>0.507806</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75231</th>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614455</td>\n",
       "      <td>0.228766</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75232</th>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.592440</td>\n",
       "      <td>0.067646</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75233 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name_length  avg_token_length  num_tokens  period_freq  dash_freq  \\\n",
       "0                9               9.0           1            0          0   \n",
       "1               12              12.0           1            0          0   \n",
       "2                8               8.0           1            0          0   \n",
       "3                9               9.0           1            0          0   \n",
       "4                8               8.0           1            0          0   \n",
       "...            ...               ...         ...          ...        ...   \n",
       "75228           10               4.5           2            0          1   \n",
       "75229           12               5.5           2            0          1   \n",
       "75230            9               9.0           1            0          0   \n",
       "75231           11              11.0           1            0          0   \n",
       "75232            9               4.0           2            0          1   \n",
       "\n",
       "       apostrophe_freq  space_freq  unigrams_cosine_sim  bigrams_cosine_sim  \\\n",
       "0                    0           0             0.664809            0.250640   \n",
       "1                    0           0             0.686625            0.353292   \n",
       "2                    0           0             0.688312            0.197139   \n",
       "3                    0           0             0.581396            0.155386   \n",
       "4                    0           0             0.463215            0.176917   \n",
       "...                ...         ...                  ...                 ...   \n",
       "75228                0           1             0.675708            0.213562   \n",
       "75229                0           1             0.770059            0.395635   \n",
       "75230                0           0             0.777063            0.507806   \n",
       "75231                0           0             0.614455            0.228766   \n",
       "75232                0           1             0.592440            0.067646   \n",
       "\n",
       "         language  \n",
       "0      Indonesian  \n",
       "1      Indonesian  \n",
       "2      Indonesian  \n",
       "3      Indonesian  \n",
       "4      Indonesian  \n",
       "...           ...  \n",
       "75228      Korean  \n",
       "75229      Korean  \n",
       "75230      Korean  \n",
       "75231      Korean  \n",
       "75232      Korean  \n",
       "\n",
       "[75233 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col = merged_df['language']\n",
    "merged_df = merged_df.select_dtypes(exclude = 'object')\n",
    "merged_df.drop('trigrams_cosine_sim', inplace = True, axis = 1)\n",
    "merged_df['language'] = label_col\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that there are no null values\n",
    "np.any(pd.isnull(merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_pickle('merged_df.pkl.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE everything after this: we will be training in individual files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "1. Clean up columns so we can combine dataframes into one (focus on making an all-Latin dataset first)\n",
    "    - do not combine in this step\n",
    "2. Frequency distributions for Latin names -> redo\n",
    "3. Add a_hat_freq\n",
    "4. Only keep numerical columns\n",
    "    - turn some categorical features -> numerical so we have more things to feed into model\n",
    "5. Add in label (language) for each dataset\n",
    "6. Combine Latin and non-Latin names to make one big dataset\n",
    "    - may need to repeat some of the above steps for non-Latin names\n",
    "7. Train test split\n",
    "8. MODEL TRAINING!\n",
    "9. Model evaluation\n",
    "\n",
    "Reminder:\n",
    "- We decided to keep period_freq, dash_freq, apostrophe_freq for now. After our first run of model training, we can remove them to see if it improves the performance\n",
    "\n",
    "**You can work on these steps out of order** (act as if the previous steps r there), but in the end we ideally want all of these steps implemented in this order.\n",
    "\n",
    "For example, you could write the code for model training and train the model on one or a few datasets. Later on, we'll just replace the variables you used with the ones containing all the languages/names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    name  class lang\n",
      "0                The Canal of the Angels      0   en\n",
      "1                      Rescue Renovation      0   en\n",
      "2       Agatha Christie: The ABC Murders      0   en\n",
      "3                            Siti Akbari      0   ar\n",
      "4                                  Stany      0   pl\n",
      "...                                  ...    ...  ...\n",
      "199995                   Robber's Bridge      0   en\n",
      "199996                       Johan Renck      0   en\n",
      "199997                      Lyle Stewart      1   en\n",
      "199998           Thomas Colclough Watson      1   en\n",
      "199999                              Gavà      0   ca\n",
      "\n",
      "[200000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#can replace file names later\n",
    "filename = os.path.join(os.getcwd(), \"company_person_name_dataset.csv\")\n",
    "#filename = os.path.join(os.getcwd(), \"Name_Of_Origin_Project-\", \"company_person_name_dataset.csv\")\n",
    "df = pd.read_csv(filename, header=0)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name_length', 'avg_token_length', 'num_tokens', 'period_freq',\n",
      "       'dash_freq', 'apostrophe_freq', 'space_freq', 'unigrams_cosine_sim',\n",
      "       'bigrams_cosine_sim', 'language'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_length</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>period_freq</th>\n",
       "      <th>dash_freq</th>\n",
       "      <th>apostrophe_freq</th>\n",
       "      <th>space_freq</th>\n",
       "      <th>unigrams_cosine_sim</th>\n",
       "      <th>bigrams_cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664809</td>\n",
       "      <td>0.250640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.686625</td>\n",
       "      <td>0.353292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.197139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.581396</td>\n",
       "      <td>0.155386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463215</td>\n",
       "      <td>0.176917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_length  avg_token_length  num_tokens  period_freq  dash_freq  \\\n",
       "0            9               9.0           1            0          0   \n",
       "1           12              12.0           1            0          0   \n",
       "2            8               8.0           1            0          0   \n",
       "3            9               9.0           1            0          0   \n",
       "4            8               8.0           1            0          0   \n",
       "\n",
       "   apostrophe_freq  space_freq  unigrams_cosine_sim  bigrams_cosine_sim  \n",
       "0                0           0             0.664809            0.250640  \n",
       "1                0           0             0.686625            0.353292  \n",
       "2                0           0             0.688312            0.197139  \n",
       "3                0           0             0.581396            0.155386  \n",
       "4                0           0             0.463215            0.176917  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = merged_df['language']\n",
    "print(merged_df.columns)\n",
    "\n",
    "X = merged_df.drop(columns = 'language', axis = 1) # oops this code is a bit redundant with before but its ok\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Training - Random Forest\n",
    "rf_classifier = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "             param_grid={&#x27;max_depth&#x27;: [None, 10, 20],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "             param_grid={&#x27;max_depth&#x27;: [None, 10, 20],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "             param_grid={'max_depth': [None, 10, 20],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 200]})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid=param_grid_rf, cv=5)\n",
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from the grid search\n",
    "best_rf_model = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Random Forest model on the test set\n",
    "rf_predictions = best_rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8428922708845618\n",
      "Random Forest Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Chinese (Characters)       0.79      0.88      0.83      2211\n",
      " Chinese (Romanized)       0.68      0.75      0.72      2096\n",
      "          Indonesian       0.73      0.80      0.76      2249\n",
      "              Korean       0.94      0.91      0.93      3844\n",
      "               Malay       0.55      0.23      0.33       582\n",
      "             Turkish       1.00      0.99      1.00      3607\n",
      "          Vietnamese       0.56      0.33      0.42       458\n",
      "\n",
      "            accuracy                           0.84     15047\n",
      "           macro avg       0.75      0.70      0.71     15047\n",
      "        weighted avg       0.84      0.84      0.84     15047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Training - SVM\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_svm = GridSearchCV(svm_classifier, param_grid=param_grid_svm, cv=5)\n",
    "grid_search_svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from the grid search\n",
    "best_svm_model = grid_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the SVM model on the test set\n",
    "svm_predictions = best_svm_model.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will leave this commented for now\n",
    "# randomizing data - idk if this is correct or necessary?\n",
    "# X, y = shuffle(X, y)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can change test data size\n",
    "# changed: 0.10 -> 0.30\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1234) \n",
    "# y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline with a TfidfVectorizer and Multinomial Naive Bayes classifier\n",
    "pipeline_nb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()) # we want this probably\n",
    "])\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid_nb = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__alpha': (1e-2, 1e-3, 1e-4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_nb = GridSearchCV(pipeline_nb, param_grid_nb, cv = 5, n_jobs = -1)\n",
    "grid_search_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from the grid search\n",
    "best_nb_model = grid_search_nb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters and best score\n",
    "# not in the other code parts but its ok\n",
    "print(\"Best Parameters: \", grid_search_nb.best_params_)\n",
    "print(\"Best Score: \", grid_search_nb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions = grid_search_nb.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "\n",
    "print(\"NB Accuracy:\", svm_accuracy)\n",
    "print(\"NB Classification Report:\")\n",
    "print(classification_report(y_test, nb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest, SVM, RNNs, Naive Bayes\n",
    "\n",
    "use gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "# rf = RandomForestClassifier(criterion = 'entropy', n_estimators = 20)\n",
    "# rf.fit(X_train, y_train)\n",
    "# rf_predictions = list(rf_20_model.predict_proba(X_test)[:,1])\n",
    "# in ML foundations we used ROC and AUC to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there r multiple types of support vector machines\n",
    "# not sure if this is correct\n",
    "# svc = svm.SVC()\n",
    "# svc.fit(X_train, y_train)\n",
    "# svc_predictions = svc.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNs - not sure if this is correct\n",
    "# mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, ... hidden_layer_sizes=(5, 2), random_state=1)\n",
    "# mlp.fit(X_train, y_train)\n",
    "# mlp_predictions = mlp.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes - there r diff types\n",
    "# this is multinomialNB, is said to be used for text classification\n",
    "# mn_nb = MultinomialNB(force_alpha=True) # idk\n",
    "# mn_nb.fit(X_train, y_train)\n",
    "# mn_nb_predictions = mn_nb.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation: precision, recall, f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "# need multiple cells, one for each evaluation\n",
    "# rf_f1 = f1_score(y_test, rf_predictions, average=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

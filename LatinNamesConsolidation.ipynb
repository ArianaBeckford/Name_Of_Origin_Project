{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file was originally for frequency distributions for Latin names!\n",
    "but we can also concat the other dfs / do model training after in the same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indo = pd.read_pickle('df_indo.pkl.gz', compression='gzip')\n",
    "df_malay = pd.read_pickle('df_malay.pkl.gz', compression='gzip')\n",
    "df_viet = pd.read_csv('viet_df.csv')\n",
    "df_cnrom = pd.read_csv('cnrom_df.csv')\n",
    "df_cnchar = pd.read_csv('cnchar_df.csv')\n",
    "df_turk = pd.read_pickle('turkish_df.pkl.gz', compression='gzip')\n",
    "df_korean = pd.read_pickle('korean_df.pkl.gz', compression='gzip') \n",
    "# also import other dfs\n",
    "\n",
    "all_dfs = [df_indo, df_malay, df_viet, df_cnrom, df_cnchar, df_turk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning up column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column names to KEEP: (10 so far)\n",
    "\n",
    "* name_length\n",
    "* avg_token_length\n",
    "* num_tokens\n",
    "* period_freq\n",
    "* dash_freq\n",
    "* apostrophe_freq\n",
    "* space_freq\n",
    "* unigrams_cosine_sim\n",
    "* bigrams_cosine_sim\n",
    "* language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # finding what columns r in viet that aren't in indo\n",
    "# print(df_indo.columns)\n",
    "# print(df_viet.columns)\n",
    "# [col for col in df_viet.columns if col not in df_indo.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk if i can generalize this lol\n",
    "# def rename_columns(df_ref, df_to_change):\n",
    "#     diff_cols = [col for col in df_to_change.columns if col not in df_ref.columns]\n",
    "#     new_names = \n",
    "#     df_to_change.rename(columns = {oldName : newName})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where you rename columns to all match\n",
    "df_viet.rename(columns = {'word_length': 'name_length'}, inplace = True)\n",
    "df_cnrom.rename(columns = {'word_length': 'name_length'}, inplace = True)\n",
    "df_cnchar.rename(columns = {'word_length': 'name_length'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Redoing frequency distributions across all Latin names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Frequency Distribution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from IndoMalay.ipynb\n",
    "\n",
    "def create_lang_char_distribution(df, col_name):\n",
    "    char_freqs = {}\n",
    "    total_num_chars = 0  # across the entire language/dataset\n",
    "\n",
    "    for name in df[col_name]:\n",
    "        for char in name:\n",
    "            if char not in char_freqs.keys():\n",
    "                char_freqs[char] = 1\n",
    "            else:\n",
    "                char_freqs[char] += 1\n",
    "            total_num_chars += 1\n",
    "\n",
    "    char_freqs_relative = dict(sorted({char: count / total_num_chars for char, count in char_freqs.items()}.items()))\n",
    "    return char_freqs_relative\n",
    "\n",
    "def initialize_all_possible_bigrams(all_possible_chars):\n",
    "    all_possible_bigrams = {}\n",
    "    for first_char in all_possible_chars:  # first character of the current bigram\n",
    "        for second_char in all_possible_chars:  # second character of the current bigram\n",
    "            all_possible_bigrams[(first_char, second_char)] = 0\n",
    "    return all_possible_bigrams\n",
    "\n",
    "def create_lang_gram_distribution(initialized_grams, df, col_name):\n",
    "    gram_freqs = initialized_grams.copy()  # need a copy otherwise initiailized_grams is changed\n",
    "    total_num_grams = 0  # across the entire language/dataset\n",
    "    \n",
    "    for grams_list in df[col_name]:\n",
    "        for gram in grams_list:\n",
    "            gram_freqs[gram] += 1\n",
    "            total_num_grams += 1\n",
    "    \n",
    "    gram_freqs_relative = {gram: count / total_num_grams for gram, count in gram_freqs.items()}\n",
    "    return gram_freqs_relative\n",
    "\n",
    "def initialize_all_possible_trigrams(all_possible_chars):\n",
    "    all_possible_trigrams = {}\n",
    "    for first_char in all_possible_chars:  # first character of the current trigram\n",
    "        for second_char in all_possible_chars:  # second character of the current trigram\n",
    "            for third_char in all_possible_chars:  # third character of the current trigram\n",
    "                all_possible_trigrams[(first_char, second_char, third_char)] = 0\n",
    "    return all_possible_trigrams\n",
    "\n",
    "def create_indiv_gram_distribution(grams_list, initialized_grams):\n",
    "    gram_freqs_relative = initialized_grams.copy()  \n",
    "    num_grams = len(grams_list)  # for this current example\n",
    "    \n",
    "    for gram in grams_list:\n",
    "        gram_freqs_relative[gram] += 1 / num_grams\n",
    "\n",
    "    return gram_freqs_relative\n",
    "\n",
    "def set_indiv_trigram_dist(trigrams_list, init_trigrams):\n",
    "    trigrams_fdist_relative = init_trigrams\n",
    "    num_grams = len(trigrams_list)\n",
    "\n",
    "    for gram in trigrams_list:\n",
    "        trigrams_fdist_relative[gram] += 1 / num_grams\n",
    "\n",
    "    return trigrams_fdist_relative\n",
    "\n",
    "# TRIGRAMS individual frequency distributions\n",
    "#df_indo['indiv_trigrams_fdist'] = df_indo.apply(lambda row: set_indiv_trigram_dist(row['trigrams'], row['indiv_trigrams_fdist']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Determining which languages use Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these lines of code to work, the datasets must have been pickled to preserve data types! `pd.csv` turns everything into strings; for example, a list of `[LATIN, LATIN, LATIN, ...]` becomes `'[LATIN, LATIN, LATIN, ...]'` (i.e., `'['` becomes a character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.2, 'I': 0.2, 'L': 0.2, 'N': 0.2, 'T': 0.2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indo_latin_percent = create_lang_char_distribution(df_indo, 'alphabet')['LATIN']\n",
    "malay_latin_percent = create_lang_char_distribution(df_malay, 'alphabet')['LATIN']\n",
    "#malay_latin_percent\n",
    "# viet_latin_percent = create_lang_char_distribution(df_viet, 'alphabet')['LATIN']\n",
    "# cnrom_latin_percent = create_lang_char_distribution(df_cnrom, 'alphabet')['LATIN']\n",
    "# cnchar_latin_percent = create_lang_char_distribution(df_cnchar, 'alphabet')['LATIN']\n",
    "turk_latin_percent = create_lang_char_distribution(df_turk, 'alphabet')\n",
    "turk_latin_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. Remaking Frequency Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add a_hat_freq and turn categorical columns into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Keeping numerical columns only for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO FOR EACH DATASET\n",
    "# # dropping non-numerical columns - this is why it would be good to have more numerical features since we don't have a lot\n",
    "# df_indo = df_indo.select_dtypes(exclude = 'object')\n",
    "# # df_indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DROP FOR WHOLE DATASET\n",
    "# # dropping trigrams for indo and malay\n",
    "# df_indo.drop('trigrams_cosine_sim', inplace = True, axis = 1)\n",
    "# df_malay.drop('trigrams_cosine_sim', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean up viet / other dfs before combining\n",
    "# df_viet.drop('trigrams', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Adding the language (label) to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indo['language'] = 'Indonesian'\n",
    "df_malay['language'] = 'Malay'\n",
    "df_viet['language'] = 'Vietnamese'\n",
    "df_cnrom['language'] = 'Chinese (Romanized)'\n",
    "df_cnchar['language'] = 'Chinese (Characters)'\n",
    "df_turk['language'] = 'Turkish'\n",
    "#df_korean['language'] = 'Korean (Romanized & Characters)'  Have to fix\n",
    "# df_indo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Combining all names to make one big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>original_fullname</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>char_ngrams</th>\n",
       "      <th>word_ngrams</th>\n",
       "      <th>name_length</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>space_freq</th>\n",
       "      <th>indiv_unigrams_fdist</th>\n",
       "      <th>indiv_bigrams_fdist</th>\n",
       "      <th>indiv_trigrams_fdist</th>\n",
       "      <th>unigrams_cosine_sim</th>\n",
       "      <th>bigrams_cosine_sim</th>\n",
       "      <th>trigrams_cosine_sim</th>\n",
       "      <th>language</th>\n",
       "      <th>id</th>\n",
       "      <th>label_tr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supriyadi</td>\n",
       "      <td>Supriyadi</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[s, u, p, r, i, y, a, d, i]</td>\n",
       "      <td>[(s, u), (u, p), (p, r), (r, i), (i, y), (y, a...</td>\n",
       "      <td>[(s, u, p), (u, p, r), (p, r, i), (r, i, y), (...</td>\n",
       "      <td>[s, u, p, r, i, y, a, d, i, (s, u), (u, p), (p...</td>\n",
       "      <td>[supriyadi]</td>\n",
       "      <td>9</td>\n",
       "      <td>9.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.664809</td>\n",
       "      <td>0.250640</td>\n",
       "      <td>0.085949</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>triyaningsih</td>\n",
       "      <td>Triyaningsih</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[t, r, i, y, a, n, i, n, g, s, i, h]</td>\n",
       "      <td>[(t, r), (r, i), (i, y), (y, a), (a, n), (n, i...</td>\n",
       "      <td>[(t, r, i), (r, i, y), (i, y, a), (y, a, n), (...</td>\n",
       "      <td>[t, r, i, y, a, n, i, n, g, s, i, h, (t, r), (...</td>\n",
       "      <td>[triyaningsih]</td>\n",
       "      <td>12</td>\n",
       "      <td>12.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.686625</td>\n",
       "      <td>0.353292</td>\n",
       "      <td>0.117226</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soerjadi</td>\n",
       "      <td>Soerjadi</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[s, o, e, r, j, a, d, i]</td>\n",
       "      <td>[(s, o), (o, e), (e, r), (r, j), (j, a), (a, d...</td>\n",
       "      <td>[(s, o, e), (o, e, r), (e, r, j), (r, j, a), (...</td>\n",
       "      <td>[s, o, e, r, j, a, d, i, (s, o), (o, e), (e, r...</td>\n",
       "      <td>[soerjadi]</td>\n",
       "      <td>8</td>\n",
       "      <td>8.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0....</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.197139</td>\n",
       "      <td>0.090295</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>undunsyah</td>\n",
       "      <td>Undunsyah</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[u, n, d, u, n, s, y, a, h]</td>\n",
       "      <td>[(u, n), (n, d), (d, u), (u, n), (n, s), (s, y...</td>\n",
       "      <td>[(u, n, d), (n, d, u), (d, u, n), (u, n, s), (...</td>\n",
       "      <td>[u, n, d, u, n, s, y, a, h, (u, n), (n, d), (d...</td>\n",
       "      <td>[undunsyah]</td>\n",
       "      <td>9</td>\n",
       "      <td>9.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.581396</td>\n",
       "      <td>0.155386</td>\n",
       "      <td>0.060083</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soeripto</td>\n",
       "      <td>Soeripto</td>\n",
       "      <td>[LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...</td>\n",
       "      <td>[s, o, e, r, i, p, t, o]</td>\n",
       "      <td>[(s, o), (o, e), (e, r), (r, i), (i, p), (p, t...</td>\n",
       "      <td>[(s, o, e), (o, e, r), (e, r, i), (r, i, p), (...</td>\n",
       "      <td>[s, o, e, r, i, p, t, o, (s, o), (o, e), (e, r...</td>\n",
       "      <td>[soeripto]</td>\n",
       "      <td>8</td>\n",
       "      <td>8.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.463215</td>\n",
       "      <td>0.176917</td>\n",
       "      <td>0.052811</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56009</th>\n",
       "      <td>nil i̇pek hülagü öztürkmen</td>\n",
       "      <td>Nil İpek Hülagü Öztürkmen</td>\n",
       "      <td>LATIN</td>\n",
       "      <td>['n', 'i', 'l', ' ', 'i', '̇', 'p', 'e', 'k', ...</td>\n",
       "      <td>[('n', 'i'), ('i', 'l'), ('l', ' '), (' ', 'i'...</td>\n",
       "      <td>[('n', 'i', 'l'), ('i', 'l', ' '), ('l', ' ', ...</td>\n",
       "      <td>['n', 'i', 'l', ' ', 'i', '̇', 'p', 'e', 'k', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>5.75</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0.11538462 0.         0.         0.03846154 ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.766280</td>\n",
       "      <td>0.250772</td>\n",
       "      <td>0.100388</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>http://www.wikidata.org/entity/Q49703809</td>\n",
       "      <td>Nil İpek Hülagü Öztürkmen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56010</th>\n",
       "      <td>fatma betül sayan kaya</td>\n",
       "      <td>Fatma Betül Sayan Kaya</td>\n",
       "      <td>LATIN</td>\n",
       "      <td>['f', 'a', 't', 'm', 'a', ' ', 'b', 'e', 't', ...</td>\n",
       "      <td>[('f', 'a'), ('a', 't'), ('t', 'm'), ('m', 'a'...</td>\n",
       "      <td>[('f', 'a', 't'), ('a', 't', 'm'), ('t', 'm', ...</td>\n",
       "      <td>['f', 'a', 't', 'm', 'a', ' ', 'b', 'e', 't', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>4.75</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0.13636364 0.         0.         0.27272727 ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.453787</td>\n",
       "      <td>0.210581</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>http://www.wikidata.org/entity/Q24230049</td>\n",
       "      <td>Fatma Betül Sayan Kaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56011</th>\n",
       "      <td>elif nur bozkurt tandoğan</td>\n",
       "      <td>Elif Nur Bozkurt Tandoğan</td>\n",
       "      <td>LATIN</td>\n",
       "      <td>['e', 'l', 'i', 'f', ' ', 'n', 'u', 'r', ' ', ...</td>\n",
       "      <td>[('e', 'l'), ('l', 'i'), ('i', 'f'), ('f', ' '...</td>\n",
       "      <td>[('e', 'l', 'i'), ('l', 'i', 'f'), ('i', 'f', ...</td>\n",
       "      <td>['e', 'l', 'i', 'f', ' ', 'n', 'u', 'r', ' ', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>5.50</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0.12 0.   0.   0.08 0.04 0.   0.04 0.04 0.04...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.820321</td>\n",
       "      <td>0.366293</td>\n",
       "      <td>0.123383</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>http://www.wikidata.org/entity/Q6053953</td>\n",
       "      <td>Elif Nur Bozkurt Tandoğan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56012</th>\n",
       "      <td>muhammed ali fatih erbakan</td>\n",
       "      <td>Muhammed Ali Fatih Erbakan</td>\n",
       "      <td>LATIN</td>\n",
       "      <td>['m', 'u', 'h', 'a', 'm', 'm', 'e', 'd', ' ', ...</td>\n",
       "      <td>[('m', 'u'), ('u', 'h'), ('h', 'a'), ('a', 'm'...</td>\n",
       "      <td>[('m', 'u', 'h'), ('u', 'h', 'a'), ('h', 'a', ...</td>\n",
       "      <td>['m', 'u', 'h', 'a', 'm', 'm', 'e', 'd', ' ', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>5.75</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0.11538462 0.         0.         0.19230769 ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.894470</td>\n",
       "      <td>0.462341</td>\n",
       "      <td>0.148177</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>http://www.wikidata.org/entity/Q6085044</td>\n",
       "      <td>Muhammed Ali Fatih Erbakan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56013</th>\n",
       "      <td>vedat ali özkan kayacı</td>\n",
       "      <td>Vedat Ali Özkan Kayacı</td>\n",
       "      <td>LATIN</td>\n",
       "      <td>['v', 'e', 'd', 'a', 't', ' ', 'a', 'l', 'i', ...</td>\n",
       "      <td>[('v', 'e'), ('e', 'd'), ('d', 'a'), ('a', 't'...</td>\n",
       "      <td>[('v', 'e', 'd'), ('e', 'd', 'a'), ('d', 'a', ...</td>\n",
       "      <td>['v', 'e', 'd', 'a', 't', ' ', 'a', 'l', 'i', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>4.75</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0.13636364 0.         0.         0.22727273 ...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]]</td>\n",
       "      <td>0.828399</td>\n",
       "      <td>0.489544</td>\n",
       "      <td>0.313943</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>http://www.wikidata.org/entity/Q6071196</td>\n",
       "      <td>Vedat Ali Özkan Kayacı</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56014 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         fullname           original_fullname  \\\n",
       "0                       supriyadi                   Supriyadi   \n",
       "1                    triyaningsih                Triyaningsih   \n",
       "2                        soerjadi                    Soerjadi   \n",
       "3                       undunsyah                   Undunsyah   \n",
       "4                        soeripto                    Soeripto   \n",
       "...                           ...                         ...   \n",
       "56009  nil i̇pek hülagü öztürkmen   Nil İpek Hülagü Öztürkmen   \n",
       "56010      fatma betül sayan kaya      Fatma Betül Sayan Kaya   \n",
       "56011   elif nur bozkurt tandoğan   Elif Nur Bozkurt Tandoğan   \n",
       "56012  muhammed ali fatih erbakan  Muhammed Ali Fatih Erbakan   \n",
       "56013      vedat ali özkan kayacı      Vedat Ali Özkan Kayacı   \n",
       "\n",
       "                                                alphabet  \\\n",
       "0      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "1      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "2      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "3      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "4      [LATIN, LATIN, LATIN, LATIN, LATIN, LATIN, LAT...   \n",
       "...                                                  ...   \n",
       "56009                                              LATIN   \n",
       "56010                                              LATIN   \n",
       "56011                                              LATIN   \n",
       "56012                                              LATIN   \n",
       "56013                                              LATIN   \n",
       "\n",
       "                                                unigrams  \\\n",
       "0                            [s, u, p, r, i, y, a, d, i]   \n",
       "1                   [t, r, i, y, a, n, i, n, g, s, i, h]   \n",
       "2                               [s, o, e, r, j, a, d, i]   \n",
       "3                            [u, n, d, u, n, s, y, a, h]   \n",
       "4                               [s, o, e, r, i, p, t, o]   \n",
       "...                                                  ...   \n",
       "56009  ['n', 'i', 'l', ' ', 'i', '̇', 'p', 'e', 'k', ...   \n",
       "56010  ['f', 'a', 't', 'm', 'a', ' ', 'b', 'e', 't', ...   \n",
       "56011  ['e', 'l', 'i', 'f', ' ', 'n', 'u', 'r', ' ', ...   \n",
       "56012  ['m', 'u', 'h', 'a', 'm', 'm', 'e', 'd', ' ', ...   \n",
       "56013  ['v', 'e', 'd', 'a', 't', ' ', 'a', 'l', 'i', ...   \n",
       "\n",
       "                                                 bigrams  \\\n",
       "0      [(s, u), (u, p), (p, r), (r, i), (i, y), (y, a...   \n",
       "1      [(t, r), (r, i), (i, y), (y, a), (a, n), (n, i...   \n",
       "2      [(s, o), (o, e), (e, r), (r, j), (j, a), (a, d...   \n",
       "3      [(u, n), (n, d), (d, u), (u, n), (n, s), (s, y...   \n",
       "4      [(s, o), (o, e), (e, r), (r, i), (i, p), (p, t...   \n",
       "...                                                  ...   \n",
       "56009  [('n', 'i'), ('i', 'l'), ('l', ' '), (' ', 'i'...   \n",
       "56010  [('f', 'a'), ('a', 't'), ('t', 'm'), ('m', 'a'...   \n",
       "56011  [('e', 'l'), ('l', 'i'), ('i', 'f'), ('f', ' '...   \n",
       "56012  [('m', 'u'), ('u', 'h'), ('h', 'a'), ('a', 'm'...   \n",
       "56013  [('v', 'e'), ('e', 'd'), ('d', 'a'), ('a', 't'...   \n",
       "\n",
       "                                                trigrams  \\\n",
       "0      [(s, u, p), (u, p, r), (p, r, i), (r, i, y), (...   \n",
       "1      [(t, r, i), (r, i, y), (i, y, a), (y, a, n), (...   \n",
       "2      [(s, o, e), (o, e, r), (e, r, j), (r, j, a), (...   \n",
       "3      [(u, n, d), (n, d, u), (d, u, n), (u, n, s), (...   \n",
       "4      [(s, o, e), (o, e, r), (e, r, i), (r, i, p), (...   \n",
       "...                                                  ...   \n",
       "56009  [('n', 'i', 'l'), ('i', 'l', ' '), ('l', ' ', ...   \n",
       "56010  [('f', 'a', 't'), ('a', 't', 'm'), ('t', 'm', ...   \n",
       "56011  [('e', 'l', 'i'), ('l', 'i', 'f'), ('i', 'f', ...   \n",
       "56012  [('m', 'u', 'h'), ('u', 'h', 'a'), ('h', 'a', ...   \n",
       "56013  [('v', 'e', 'd'), ('e', 'd', 'a'), ('d', 'a', ...   \n",
       "\n",
       "                                             char_ngrams     word_ngrams  \\\n",
       "0      [s, u, p, r, i, y, a, d, i, (s, u), (u, p), (p...     [supriyadi]   \n",
       "1      [t, r, i, y, a, n, i, n, g, s, i, h, (t, r), (...  [triyaningsih]   \n",
       "2      [s, o, e, r, j, a, d, i, (s, o), (o, e), (e, r...      [soerjadi]   \n",
       "3      [u, n, d, u, n, s, y, a, h, (u, n), (n, d), (d...     [undunsyah]   \n",
       "4      [s, o, e, r, i, p, t, o, (s, o), (o, e), (e, r...      [soeripto]   \n",
       "...                                                  ...             ...   \n",
       "56009  ['n', 'i', 'l', ' ', 'i', '̇', 'p', 'e', 'k', ...             NaN   \n",
       "56010  ['f', 'a', 't', 'm', 'a', ' ', 'b', 'e', 't', ...             NaN   \n",
       "56011  ['e', 'l', 'i', 'f', ' ', 'n', 'u', 'r', ' ', ...             NaN   \n",
       "56012  ['m', 'u', 'h', 'a', 'm', 'm', 'e', 'd', ' ', ...             NaN   \n",
       "56013  ['v', 'e', 'd', 'a', 't', ' ', 'a', 'l', 'i', ...             NaN   \n",
       "\n",
       "       name_length  avg_token_length  ...  space_freq  \\\n",
       "0                9              9.00  ...           0   \n",
       "1               12             12.00  ...           0   \n",
       "2                8              8.00  ...           0   \n",
       "3                9              9.00  ...           0   \n",
       "4                8              8.00  ...           0   \n",
       "...            ...               ...  ...         ...   \n",
       "56009           23              5.75  ...           3   \n",
       "56010           19              4.75  ...           3   \n",
       "56011           22              5.50  ...           3   \n",
       "56012           23              5.75  ...           3   \n",
       "56013           19              4.75  ...           3   \n",
       "\n",
       "                                    indiv_unigrams_fdist  \\\n",
       "0      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111,...   \n",
       "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333...   \n",
       "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0....   \n",
       "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111,...   \n",
       "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "...                                                  ...   \n",
       "56009  [[0.11538462 0.         0.         0.03846154 ...   \n",
       "56010  [[0.13636364 0.         0.         0.27272727 ...   \n",
       "56011  [[0.12 0.   0.   0.08 0.04 0.   0.04 0.04 0.04...   \n",
       "56012  [[0.11538462 0.         0.         0.19230769 ...   \n",
       "56013  [[0.13636364 0.         0.         0.22727273 ...   \n",
       "\n",
       "                                     indiv_bigrams_fdist  \\\n",
       "0      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "...                                                  ...   \n",
       "56009                          [[0. 0. 0. ... 0. 0. 0.]]   \n",
       "56010                          [[0. 0. 0. ... 0. 0. 0.]]   \n",
       "56011                          [[0. 0. 0. ... 0. 0. 0.]]   \n",
       "56012                          [[0. 0. 0. ... 0. 0. 0.]]   \n",
       "56013                          [[0. 0. 0. ... 0. 0. 0.]]   \n",
       "\n",
       "                                    indiv_trigrams_fdist  unigrams_cosine_sim  \\\n",
       "0      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.664809   \n",
       "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.686625   \n",
       "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.688312   \n",
       "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.581396   \n",
       "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...             0.463215   \n",
       "...                                                  ...                  ...   \n",
       "56009                          [[0. 0. 0. ... 0. 0. 0.]]             0.766280   \n",
       "56010                          [[0. 0. 0. ... 0. 0. 0.]]             0.812166   \n",
       "56011                          [[0. 0. 0. ... 0. 0. 0.]]             0.820321   \n",
       "56012                          [[0. 0. 0. ... 0. 0. 0.]]             0.894470   \n",
       "56013                          [[0. 0. 0. ... 0. 0. 0.]]             0.828399   \n",
       "\n",
       "       bigrams_cosine_sim trigrams_cosine_sim    language  \\\n",
       "0                0.250640            0.085949  Indonesian   \n",
       "1                0.353292            0.117226  Indonesian   \n",
       "2                0.197139            0.090295  Indonesian   \n",
       "3                0.155386            0.060083  Indonesian   \n",
       "4                0.176917            0.052811  Indonesian   \n",
       "...                   ...                 ...         ...   \n",
       "56009            0.250772            0.100388     Turkish   \n",
       "56010            0.453787            0.210581     Turkish   \n",
       "56011            0.366293            0.123383     Turkish   \n",
       "56012            0.462341            0.148177     Turkish   \n",
       "56013            0.489544            0.313943     Turkish   \n",
       "\n",
       "                                             id                    label_tr  \n",
       "0                                           NaN                         NaN  \n",
       "1                                           NaN                         NaN  \n",
       "2                                           NaN                         NaN  \n",
       "3                                           NaN                         NaN  \n",
       "4                                           NaN                         NaN  \n",
       "...                                         ...                         ...  \n",
       "56009  http://www.wikidata.org/entity/Q49703809   Nil İpek Hülagü Öztürkmen  \n",
       "56010  http://www.wikidata.org/entity/Q24230049      Fatma Betül Sayan Kaya  \n",
       "56011   http://www.wikidata.org/entity/Q6053953   Elif Nur Bozkurt Tandoğan  \n",
       "56012   http://www.wikidata.org/entity/Q6085044  Muhammed Ali Fatih Erbakan  \n",
       "56013   http://www.wikidata.org/entity/Q6071196      Vedat Ali Özkan Kayacı  \n",
       "\n",
       "[56014 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see from output, we need the columns in the concatenated df (in this case, viet) to match\n",
    "# it's okay if some values are NaN bc we'll drop all non-numerical columns anyway\n",
    "merged_df = pd.concat(all_dfs, ignore_index = True, join = 'outer')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Keeping numerical columns only for each dataset (Same process as step 4, except we don't have to repeat lines of code...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_length</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>period_freq</th>\n",
       "      <th>dash_freq</th>\n",
       "      <th>apostrophe_freq</th>\n",
       "      <th>space_freq</th>\n",
       "      <th>unigrams_cosine_sim</th>\n",
       "      <th>bigrams_cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664809</td>\n",
       "      <td>0.250640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.686625</td>\n",
       "      <td>0.353292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.197139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.581396</td>\n",
       "      <td>0.155386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463215</td>\n",
       "      <td>0.176917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56009</th>\n",
       "      <td>23</td>\n",
       "      <td>5.75</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.766280</td>\n",
       "      <td>0.250772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56010</th>\n",
       "      <td>19</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.453787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56011</th>\n",
       "      <td>22</td>\n",
       "      <td>5.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.820321</td>\n",
       "      <td>0.366293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56012</th>\n",
       "      <td>23</td>\n",
       "      <td>5.75</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.894470</td>\n",
       "      <td>0.462341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56013</th>\n",
       "      <td>19</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.828399</td>\n",
       "      <td>0.489544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56014 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name_length  avg_token_length  num_tokens  period_freq  dash_freq  \\\n",
       "0                9              9.00           1            0          0   \n",
       "1               12             12.00           1            0          0   \n",
       "2                8              8.00           1            0          0   \n",
       "3                9              9.00           1            0          0   \n",
       "4                8              8.00           1            0          0   \n",
       "...            ...               ...         ...          ...        ...   \n",
       "56009           23              5.75           4            0          0   \n",
       "56010           19              4.75           4            0          0   \n",
       "56011           22              5.50           4            0          0   \n",
       "56012           23              5.75           4            0          0   \n",
       "56013           19              4.75           4            0          0   \n",
       "\n",
       "       apostrophe_freq  space_freq  unigrams_cosine_sim  bigrams_cosine_sim  \n",
       "0                  0.0           0             0.664809            0.250640  \n",
       "1                  0.0           0             0.686625            0.353292  \n",
       "2                  0.0           0             0.688312            0.197139  \n",
       "3                  0.0           0             0.581396            0.155386  \n",
       "4                  0.0           0             0.463215            0.176917  \n",
       "...                ...         ...                  ...                 ...  \n",
       "56009              NaN           3             0.766280            0.250772  \n",
       "56010              NaN           3             0.812166            0.453787  \n",
       "56011              NaN           3             0.820321            0.366293  \n",
       "56012              NaN           3             0.894470            0.462341  \n",
       "56013              NaN           3             0.828399            0.489544  \n",
       "\n",
       "[56014 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.select_dtypes(exclude = 'object')\n",
    "merged_df.drop('trigrams_cosine_sim', inplace = True, axis = 1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "1. Clean up columns so we can combine dataframes into one (focus on making an all-Latin dataset first)\n",
    "    - do not combine in this step\n",
    "2. Frequency distributions for Latin names -> redo\n",
    "3. Add a_hat_freq\n",
    "4. Only keep numerical columns\n",
    "    - turn some categorical features -> numerical so we have more things to feed into model\n",
    "5. Add in label (language) for each dataset\n",
    "6. Combine Latin and non-Latin names to make one big dataset\n",
    "    - may need to repeat some of the above steps for non-Latin names\n",
    "7. Train test split\n",
    "8. MODEL TRAINING!\n",
    "9. Model evaluation\n",
    "\n",
    "Reminder:\n",
    "- We decided to keep period_freq, dash_freq, apostrophe_freq for now. After our first run of model training, we can remove them to see if it improves the performance\n",
    "\n",
    "**You can work on these steps out of order** (act as if the previous steps r there), but in the end we ideally want all of these steps implemented in this order.\n",
    "\n",
    "For example, you could write the code for model training and train the model on one or a few datasets. Later on, we'll just replace the variables you used with the ones containing all the languages/names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can replace file names later\n",
    "#filename = os.path.join(os.getcwd(), \"Name_Of_Origin_Project-\", \"company.csv\")\n",
    "#df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = merged_df['language']\n",
    "#X = merged_df.drop(columns = 'language', axis = 1)\n",
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can change test data size\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest, SVM, RNNs, Naive Bayes\n",
    "\n",
    "use gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "# rf = RandomForestClassifier(criterion = 'entropy', n_estimators = 20)\n",
    "# rf.fit(X_train, y_train)\n",
    "# rf_predictions = list(rf_20_model.predict_proba(X_test)[:,1])\n",
    "# in ML foundations we used ROC and AUC to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there r multiple types of support vector machines\n",
    "# not sure if this is correct\n",
    "# svc = svm.SVC()\n",
    "# svc.fit(X_train, y_train)\n",
    "# svc_predictions = svc.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNs - not sure if this is correct\n",
    "# mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, ... hidden_layer_sizes=(5, 2), random_state=1)\n",
    "# mlp.fit(X_train, y_train)\n",
    "# mlp_predictions = mlp.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes - there r diff types\n",
    "# this is multinomialNB, is said to be used for text classification\n",
    "# mn_nb = MultinomialNB(force_alpha=True) # idk\n",
    "# mn_nb.fit(X_train, y_train)\n",
    "# mn_nb_predictions = mn_nb.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation: precision, recall, f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "# need multiple cells, one for each evaluation\n",
    "# rf_f1 = f1_score(y_test, rf_predictions, average=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
